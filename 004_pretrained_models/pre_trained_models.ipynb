{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from nets import vgg\n",
    "from utils.utils import *\n",
    "from utils.tf_utils import *\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', '002_image_classification'))\n",
    "from cifar10_loader import CIFAR10_loader\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "image_size = vgg.vgg_16.default_image_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 딥 러닝이 여러 분야에서 높은 성능으로 각광을 받고 있지만, 이는 ImageNet과 같은 빅 데이터의 출현에 힘입어 이뤄진 일로 딥 러닝으로 높은 성능을 내기 위해서는 충분한 양의 학습 데이터가 필요하다. 특히, detection, segmentation 등의 task는 classification보다 고수준의 annotation이 필요하기에 충분한 데이터를 모은다는 건 굉장한 노력과 비용을 필요로 한다. 하지만, classification을 위해 ImageNet에서 학습 된 네트워크는 물체를 컴퓨터가 인식하기에 적합한 feature를 추출하므로 다른 task에서 좋은 시작점으로 사용될 수 있고, 결과적으로 상대적으로 적은 데이터로 높은 성능을 낼 수 있는 모델을 학습하는 데 큰 도움이 된다. <br><br>\n",
    " 이번 튜토리얼에서는 ImageNet에서 학습된 vgg 16-layer 네트워크를 선언하고 checkpoint로부터 weight를 가져오는 방법을 공부해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pre-trained model (vgg-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'vgg_models'\n",
    "data_url = 'http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz'\n",
    "maybe_download_and_extract(data_url, data_dir, 'vgg_16.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "vgg 네트워크를 선언하고, checkpoint로부터 가져올 weight를 정해보자.\n",
    "vgg 네트워크를 스스로 선언해도 되지만, 그러기 위해서는 checkpoint에 정의된 variable들과 name이 같도록 선언해야하므로 여기에서는 TF-Slim에서 제공하는 vgg 네트워크 선언 함수 (vgg.vgg_16())을 사용하여 네트워크를 정의하고, tf.contrib.slim.get_variables_to_restore()을 통해 checkpoint로부터 가져 올 weight를 정하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> The list of variables to be restored:\n",
      "vgg_16/conv1/conv1_1/weights\n",
      "vgg_16/conv1/conv1_1/biases\n",
      "vgg_16/conv1/conv1_2/weights\n",
      "vgg_16/conv1/conv1_2/biases\n",
      "vgg_16/conv2/conv2_1/weights\n",
      "vgg_16/conv2/conv2_1/biases\n",
      "vgg_16/conv2/conv2_2/weights\n",
      "vgg_16/conv2/conv2_2/biases\n",
      "vgg_16/conv3/conv3_1/weights\n",
      "vgg_16/conv3/conv3_1/biases\n",
      "vgg_16/conv3/conv3_2/weights\n",
      "vgg_16/conv3/conv3_2/biases\n",
      "vgg_16/conv3/conv3_3/weights\n",
      "vgg_16/conv3/conv3_3/biases\n",
      "vgg_16/conv4/conv4_1/weights\n",
      "vgg_16/conv4/conv4_1/biases\n",
      "vgg_16/conv4/conv4_2/weights\n",
      "vgg_16/conv4/conv4_2/biases\n",
      "vgg_16/conv4/conv4_3/weights\n",
      "vgg_16/conv4/conv4_3/biases\n",
      "vgg_16/conv5/conv5_1/weights\n",
      "vgg_16/conv5/conv5_1/biases\n",
      "vgg_16/conv5/conv5_2/weights\n",
      "vgg_16/conv5/conv5_2/biases\n",
      "vgg_16/conv5/conv5_3/weights\n",
      "vgg_16/conv5/conv5_3/biases\n",
      "vgg_16/fc6/weights\n",
      "vgg_16/fc6/biases\n",
      "vgg_16/fc7/weights\n",
      "vgg_16/fc7/biases\n",
      "\n",
      "===> The list of variables to be learned:\n",
      "vgg_16/fc6/weights\n",
      "vgg_16/fc6/biases\n",
      "vgg_16/fc7/weights\n",
      "vgg_16/fc7/biases\n",
      "vgg_16/fc8/weights\n",
      "vgg_16/fc8/biases\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "batch_size = 100\n",
    "checkpoint_path = 'vgg_models/vgg_16.ckpt'\n",
    "\n",
    "g = tf.Graph()\n",
    "\n",
    "# input placeholders\n",
    "images = tf.placeholder(dtype=tf.float32, shape=[batch_size, image_size, image_size, 3],\n",
    "                           name='images')\n",
    "labels = tf.placeholder(dtype=tf.int64, shape=[batch_size], name='labels')\n",
    "\n",
    "# Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "    # 10 classes instead of 1001.\n",
    "    logits, _ = vgg.vgg_16(images, num_classes=num_classes, is_training=True)\n",
    "\n",
    "# Before defining remaining layers (softmax, optimizer), selecting the\n",
    "# variables to be restored\n",
    "exclude_layers = ['vgg_16/fc8']\n",
    "#exclude_layers = ['vgg_16/fc8'] # when discaring only the last classification layer\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude=exclude_layers)\n",
    "print('===> The list of variables to be restored:')\n",
    "for i in variables_to_restore: print(i.op.name)\n",
    "\n",
    "\"\"\"\n",
    "# Below code is equivalent to slim.get_variables_to_restore()\n",
    "exclusions = [scope.strip() for scope in exclude_layers]\n",
    "\n",
    "variables_to_restore = []\n",
    "for var in tf.global_variables():\n",
    "    excluded = False\n",
    "    for exclusion in exclusions:\n",
    "        if var.op.name.startswith(exclusion):\n",
    "            excluded = True\n",
    "            break\n",
    "    if not excluded:\n",
    "        variables_to_restore.append(var)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "probabilities = tf.nn.softmax(logits)\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "\n",
    "# TODO: select variables to be learned\n",
    "fix_layers = ['vgg_16/conv1', 'vgg_16/conv2', 'vgg_16/conv3', 'vgg_16/conv4', 'vgg_16/conv5']\n",
    "variables_to_learn = slim.get_variables_to_restore(exclude=fix_layers)\n",
    "#variables_to_learn = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "print('\\n===> The list of variables to be learned:')\n",
    "for i in variables_to_learn: print(i.op.name)\n",
    "\n",
    "# Specify the optimizer and create the train op:\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "train_op = optimizer.minimize(loss, var_list=variables_to_learn)\n",
    "\n",
    "global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크를 선언하였으니 tf.train.Saver()를 이용하여 checkpoint로부터 weight를 읽어오자.<br>\n",
    "위에서 convolutional layer들의 weight만 읽어오도록 하였으므로 아래에서 처음엔 모든 weight를 랜덤으로 초기화한 후, vgg checkpoint로부터 weight를 읽어와서 conv1_1과 fc7의 weight가 어떻게 프린트되는 지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the saver with variables to be restored\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "# Open the session\n",
    "sess = tf.Session()\n",
    "conv1_1_weights = tf.get_default_graph().get_tensor_by_name('vgg_16/conv1/conv1_1/weights:0')\n",
    "fc7_weights = tf.get_default_graph().get_tensor_by_name('vgg_16/fc7/weights:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of conv1_1:\n",
      "[-0.07034407  0.09699172  0.05994357  0.01191355  0.0790222  -0.05156257\n",
      "  0.07133409  0.05756056  0.0888675  -0.05755238]\n",
      "Weight of fc7:\n",
      "[ 0.00698913 -0.00441733 -0.02221106 -0.02669755  0.01187419 -0.00269974\n",
      " -0.00629738 -0.00543988  0.00157767  0.02522648]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "rand_conv1_1 = conv1_1_weights[:,:,:,:].eval(session=sess)\n",
    "rand_fc7 = fc7_weights[:,:,:,:].eval(session=sess)\n",
    "\n",
    "print('Weight of conv1_1:')\n",
    "print(rand_conv1_1[1,1,1,:10])\n",
    "print('Weight of fc7:')\n",
    "print(rand_fc7[0,0,0,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the weights from vgg-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of conv1_1:\n",
      "[ 0.04063221  0.06581022  0.2203114  -0.42466447  0.20586449 -0.23609307\n",
      " -0.04312737 -0.10727409 -0.33554825 -0.09185937]\n",
      "Weight of fc7:\n",
      "[ 0.00390148 -0.00180807  0.00136159 -0.00262455 -0.00475213 -0.00220464\n",
      " -0.00268095  0.00679421 -0.00499181  0.00407928]\n"
     ]
    }
   ],
   "source": [
    "restorer.restore(sess, save_path=checkpoint_path)\n",
    "vgg_conv1_1 = conv1_1_weights[:,:,:,:].eval(session=sess)\n",
    "vgg_fc7 = fc7_weights[:,:,:,:].eval(session=sess)\n",
    "\n",
    "print('Weight of conv1_1:')\n",
    "print(vgg_conv1_1[1,1,1,:10])\n",
    "print('Weight of fc7:')\n",
    "print(vgg_fc7[0,0,0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('difference in conv1_1: ', 4.4200602)\n",
      "('difference in fc7: ', 0.0)\n"
     ]
    }
   ],
   "source": [
    "print('difference in conv1_1: ',np.sum(rand_conv1_1 - vgg_conv1_1))\n",
    "print('difference in fc7: ', np.sum(rand_fc7 - vgg_fc7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model in CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader = CIFAR10_loader()\n",
    "class_names = loader.get_class_names()\n",
    "\n",
    "# Model parameters\n",
    "init_from = '' # checkpoint path\n",
    "save_path = 'cifar10_checkpoints/cifar10_cnn'\n",
    "if not os.path.exists('cifar10_checkpoints'): os.makedirs('cifar10_checkpoints')\n",
    "num_epochs = 10\n",
    "iteration_per_epoch = int(math.floor(loader.get_num_train_examples() / batch_size))\n",
    "save_checkpoint_frequency = 250\n",
    "print_frequency = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Epoch 10 iteration - Loss (0.145) Accuracy (0.930)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_10\n",
      "1 Epoch 20 iteration - Loss (0.114) Accuracy (0.970)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-38c6dcb55b01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Save checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_checkpoint_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mie\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0miteration_per_epoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mii\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saved checkpoint %s_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mie\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0miteration_per_epoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mii\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1372\u001b[0m       model_checkpoint_path = sess.run(\n\u001b[0;32m   1373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m           {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1375\u001b[0m       \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "saver = tf.train.Saver()\n",
    "for ie in range(num_epochs):\n",
    "    for ii in range(iteration_per_epoch):\n",
    "        # Load a batch data\n",
    "        batch = loader.get_batch(batch_size, 'train', (224,224))\n",
    "\n",
    "        # Run the optimizer\n",
    "        _ = sess.run([train_op], feed_dict={images:batch['images'],\n",
    "                                            labels:batch['labels']})\n",
    "\n",
    "        # Print the accuracy and loss of current batch data\n",
    "        if (ii+1) % print_frequency == 0:\n",
    "            batch_loss, batch_prob = sess.run([loss, probabilities], \n",
    "                                             feed_dict={images:batch['images'],\n",
    "                                                        labels:batch['labels']})\n",
    "            pred_labels = np.argmax(batch_prob, axis=1)\n",
    "            batch_loss = np.mean(batch_loss)\n",
    "            batch_acc =np.mean(np.equal(pred_labels, batch['labels']))\n",
    "            print('%d Epoch %d iteration - Loss (%.3f) Accuracy (%.3f)'\n",
    "                      %(ie+1, ii+1, batch_loss, batch_acc))\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (ii+1) % save_checkpoint_frequency == 0:\n",
    "            saver.save(sess, save_path=save_path, global_step=ie*iteration_per_epoch + ii + 1)\n",
    "            print('Saved checkpoint %s_%d' % (save_path, ie*iteration_per_epoch + ii + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last checkpoint path is cifar10_checkpoints/cifar10_cnn-10\n",
      "Model is restored from cifar10_checkpoints/cifar10_cnn-10\n",
      "10/100 done\n",
      "20/100 done\n",
      "30/100 done\n",
      "40/100 done\n",
      "50/100 done\n",
      "60/100 done\n",
      "70/100 done\n",
      "80/100 done\n",
      "90/100 done\n",
      "100/100 done\n",
      "Test accuracy: 83.45%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "iteration_per_epoch = int(math.floor(loader.get_num_test_examples() / batch_size))\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir='cifar10_checkpoints/')\n",
    "print('Last checkpoint path is %s' % (checkpoint_path))\n",
    "\n",
    "# Load the checkpoint or initialize the variables\n",
    "saver.restore(sess, save_path=checkpoint_path)\n",
    "print('Model is restored from %s' % checkpoint_path)\n",
    "\n",
    "# Evaluate the model\n",
    "loader.reset()\n",
    "ii = 1\n",
    "num_correct = 0\n",
    "num_examples = 0\n",
    "while True:\n",
    "    # Load a batch data\n",
    "    batch = loader.get_batch(batch_size, 'test', (224,224))\n",
    "    if batch['wrapped']: break\n",
    "\n",
    "    # Compute the correct numbers\n",
    "    batch_prob = sess.run(probabilities, feed_dict={images:batch['images'],\n",
    "                                                      labels:batch['labels']})\n",
    "\n",
    "    pred_labels = np.argmax(batch_prob, axis=1)\n",
    "    batch_correct_num = np.sum(np.equal(pred_labels, batch['labels']))\n",
    "    num_correct += batch_correct_num\n",
    "    num_examples += batch_size\n",
    "    \n",
    "    if (ii+1) % 10 == 0:\n",
    "        print('%d/%d done' % (ii+1, iteration_per_epoch))\n",
    "    ii += 1\n",
    "print('Test accuracy: %.2f%%' % (num_correct / num_examples * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 함수는 nets/vgg.py에 정의된 vgg 16-layer 네트워크 정의 함수로 참고용으로 첨부하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_16(inputs,\n",
    "           num_classes=1000,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='vgg_16',\n",
    "           fc_conv_padding='VALID'):\n",
    "    \"\"\"Oxford Net VGG 16-Layers version D Example.\n",
    "    Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "    Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "    fc_conv_padding: the type of padding to use for the fully connected layer\n",
    "      that is implemented as a convolutional layer. Use 'SAME' padding if you\n",
    "      are applying the network in a fully convolutional manner and want to\n",
    "      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n",
    "      the output prediction map will be (input / 32) - 6 in case of 'VALID' padding.\n",
    "    Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n",
    "        end_points_collection = sc.name + '_end_points'\n",
    "        # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                            outputs_collections=end_points_collection):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "            # Use conv2d instead of fully_connected layers.\n",
    "            net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                             scope='dropout6')\n",
    "            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                             scope='dropout7')\n",
    "            net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                            activation_fn=None,\n",
    "                            normalizer_fn=None,\n",
    "                            scope='fc8')\n",
    "            \n",
    "        # Convert end_points_collection into a end_point dict.\n",
    "        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "        if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "        return net, end_points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
