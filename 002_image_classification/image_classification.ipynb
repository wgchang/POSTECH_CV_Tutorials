{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# own python codes\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "from utils import *\n",
    "from tf_utils import *\n",
    "from cifar10_loader import CIFAR10_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TensorFlow tutorial is written based on https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10, which is the tutorial for the image classification task on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-python.tar.gz 100.0%('Successfully downloaded', 'cifar-10-python.tar.gz', 170498071, 'bytes.')\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'cifar10_data'\n",
    "data_url = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "maybe_download_and_extract(data_url, data_dir, 'cifar-10-batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loader and check the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader = CIFAR10_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for constructing model (graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get variable initialization function\n",
    "initializer = get_initializer('normal', **{'stddev':0.01})\n",
    "\n",
    "def build_inputs(batch_size, image_size):\n",
    "    \"\"\" Construct input for CIFAR evaluation using the Reader ops.\n",
    "    Args:\n",
    "        batch_size: Batch size.\n",
    "        image_size: Image size.\n",
    "    Returns:\n",
    "        images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "        labels: Labels. 1D tensor of [batch_size] size.\n",
    "    \"\"\"\n",
    "    images = tf.placeholder(dtype=tf.float32, shape=[batch_size, image_size, image_size, 3],\n",
    "                           name='images')\n",
    "    labels = tf.placeholder(dtype=tf.float32, shape=[batch_size], name='labels')\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def build_model(images, batch_size):\n",
    "    \"\"\" Build the CIFAR-10 model.\n",
    "    Args:\n",
    "        images: Images returned from build_inputs(). 4-D tensor.\n",
    "    Returns:\n",
    "        Logits.\n",
    "    \"\"\"\n",
    "    \n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        conv1 = get_conv2D_layer(images, 3, 64, 5, 1, initializer, 0.0, \n",
    "                                 'relu', scope, True)\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        conv2 = get_conv2D_layer(norm1, 64, 64, 5, 1, initializer, 0.0, \n",
    "                                 'relu', scope, True)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "    # fc1\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        fc1 = get_fully_connected_layer(pool2, -1, 384, initializer, 0.004, 'relu', \n",
    "                                        True, batch_size, scope, True)\n",
    "\n",
    "    # fc2\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        fc2 = get_fully_connected_layer(fc1, 384, 192, initializer, 0.004, 'relu', \n",
    "                                        False, -1, scope, True)\n",
    "\n",
    "    # We don't apply softmax here because tf.nn.sparse_softmax_cross_entropy_with_logits \n",
    "    # accepts the unscaled logits and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('logits') as scope:\n",
    "        logits = get_fully_connected_layer(fc2, 192, loader.get_num_classes(), initializer, \n",
    "                                           0.0, 'None', False, -1, scope, True)\n",
    "\n",
    "    return logits\n",
    "\n",
    "def build_loss(logits, labels):\n",
    "    \"\"\" Add L2Loss to all the trainable variables.\n",
    "    Add summary for \"Loss\" and \"Loss/avg\".\n",
    "    Args:\n",
    "        logits: Logits from build_model().\n",
    "        labels: Labels from build_inputs(). 1-D tensor.\n",
    "    Returns:\n",
    "        Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    # Calculate the average cross entropy loss across the batch.\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    # The total loss is defined as the cross entropy loss plus all \n",
    "    # of the weight decay terms (L2 loss).\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "def build_train_operation(total_loss, params):\n",
    "    \"\"\" Train CIFAR-10 model and create an optimizer.\n",
    "    Args:\n",
    "        total_loss: Total loss from loss().\n",
    "        params: parameters for exponential decaying. \n",
    "                The learning rate is computed by \n",
    "                decayed_learning_rate = initial_lr * \n",
    "                    decay_rate ^ (global_step / decay_steps)\n",
    "            - initial_lr: initial learning rate\n",
    "            - decay_step: decay step\n",
    "            - decay_rate: decay rate\n",
    "    Returns:\n",
    "        train_op: op for training.\n",
    "        global_step: global step counting iteration\n",
    "    \"\"\"\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "    lr = tf.train.exponential_decay(params['initial_lr'],\n",
    "                                    global_step,\n",
    "                                    params['decay_step'],\n",
    "                                    params['decay_rate'],\n",
    "                                    staircase=True)\n",
    "    \n",
    "    # Create the optimizer which will minimize the loss.\n",
    "    train_op = tf.train.AdamOptimizer(lr).minimize(total_loss, global_step=global_step)\n",
    "\n",
    "    return train_op, global_step\n",
    "\n",
    "def build_accuracy(logits, labels):\n",
    "    \"\"\" Accuarcy computed by\n",
    "        accuracy = # of correct examples / # of total examples\n",
    "    Args:\n",
    "        logits: Logits from build_model().\n",
    "        labels: Labels from build_model().\n",
    "    Returns:\n",
    "        accuracy: Accuracy.\n",
    "        correct_num: The number of corrected examples\n",
    "    \"\"\"\n",
    "    pred_labels = tf.argmax(logits, axis=1)\n",
    "    correct_prediction = tf.equal(pred_labels, tf.cast(labels, tf.int64))\n",
    "    correct_num = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return accuracy, correct_num\n",
    "    \n",
    "    \n",
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\" Add summaries for losses in CIFAR-10 model.\n",
    "    Generates moving average for all losses and associated summaries for\n",
    "    visualizing the performance of the network.\n",
    "    Args:\n",
    "        total_loss: Total loss from loss().\n",
    "    Returns:\n",
    "        loss_averages_op: op for generating moving averages of losses.\n",
    "    \"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "init_from = '' # checkpoint path\n",
    "save_path = 'cifar10_checkpoints/cifar10_cnn'\n",
    "if not os.path.exists('cifar10_checkpoints'): os.makedirs('cifar10_checkpoints')\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "iteration_per_epoch = int(math.floor(loader.get_num_train_examples() / batch_size))\n",
    "save_checkpoint_frequency = 2500\n",
    "print_frequency = 100\n",
    "\n",
    "# learning decay rate parameters\n",
    "lr_params = {}\n",
    "lr_params['initial_lr'] = 0.001\n",
    "lr_params['decay_step'] = 1500 # 3 epoch assuming that the batch size is 100\n",
    "lr_params['decay_rate'] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Epoch 499 iteration - Loss (1.643) Accuracy (0.470)\n",
      "1 Epoch 499 iteration - Loss (1.475) Accuracy (0.520)\n",
      "2 Epoch 499 iteration - Loss (1.354) Accuracy (0.610)\n",
      "3 Epoch 499 iteration - Loss (1.231) Accuracy (0.620)\n",
      "4 Epoch 499 iteration - Loss (1.168) Accuracy (0.610)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_2500\n",
      "5 Epoch 499 iteration - Loss (1.125) Accuracy (0.610)\n",
      "6 Epoch 499 iteration - Loss (1.025) Accuracy (0.680)\n",
      "7 Epoch 499 iteration - Loss (0.950) Accuracy (0.710)\n",
      "8 Epoch 499 iteration - Loss (0.888) Accuracy (0.730)\n",
      "9 Epoch 499 iteration - Loss (0.816) Accuracy (0.740)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_5000\n",
      "10 Epoch 499 iteration - Loss (0.777) Accuracy (0.780)\n",
      "11 Epoch 499 iteration - Loss (0.740) Accuracy (0.780)\n",
      "12 Epoch 499 iteration - Loss (0.727) Accuracy (0.790)\n",
      "13 Epoch 499 iteration - Loss (0.697) Accuracy (0.800)\n",
      "14 Epoch 499 iteration - Loss (0.667) Accuracy (0.820)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_7500\n",
      "15 Epoch 499 iteration - Loss (0.651) Accuracy (0.810)\n",
      "16 Epoch 499 iteration - Loss (0.645) Accuracy (0.810)\n",
      "17 Epoch 499 iteration - Loss (0.646) Accuracy (0.810)\n",
      "18 Epoch 499 iteration - Loss (0.621) Accuracy (0.820)\n",
      "19 Epoch 499 iteration - Loss (0.603) Accuracy (0.810)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_10000\n",
      "20 Epoch 499 iteration - Loss (0.576) Accuracy (0.820)\n",
      "21 Epoch 499 iteration - Loss (0.549) Accuracy (0.850)\n",
      "22 Epoch 499 iteration - Loss (0.539) Accuracy (0.870)\n",
      "23 Epoch 499 iteration - Loss (0.527) Accuracy (0.880)\n",
      "24 Epoch 499 iteration - Loss (0.573) Accuracy (0.860)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_12500\n",
      "25 Epoch 499 iteration - Loss (0.547) Accuracy (0.880)\n",
      "26 Epoch 499 iteration - Loss (0.519) Accuracy (0.880)\n",
      "27 Epoch 499 iteration - Loss (0.518) Accuracy (0.900)\n",
      "28 Epoch 499 iteration - Loss (0.488) Accuracy (0.900)\n",
      "29 Epoch 499 iteration - Loss (0.471) Accuracy (0.920)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_15000\n",
      "30 Epoch 499 iteration - Loss (0.467) Accuracy (0.910)\n",
      "31 Epoch 499 iteration - Loss (0.449) Accuracy (0.910)\n",
      "32 Epoch 499 iteration - Loss (0.442) Accuracy (0.900)\n",
      "33 Epoch 499 iteration - Loss (0.436) Accuracy (0.900)\n",
      "34 Epoch 499 iteration - Loss (0.423) Accuracy (0.910)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_17500\n",
      "35 Epoch 499 iteration - Loss (0.409) Accuracy (0.920)\n",
      "36 Epoch 499 iteration - Loss (0.407) Accuracy (0.920)\n",
      "37 Epoch 499 iteration - Loss (0.396) Accuracy (0.930)\n",
      "38 Epoch 499 iteration - Loss (0.387) Accuracy (0.930)\n",
      "39 Epoch 499 iteration - Loss (0.384) Accuracy (0.940)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_20000\n",
      "40 Epoch 499 iteration - Loss (0.376) Accuracy (0.940)\n",
      "41 Epoch 499 iteration - Loss (0.370) Accuracy (0.940)\n",
      "42 Epoch 499 iteration - Loss (0.375) Accuracy (0.940)\n",
      "43 Epoch 499 iteration - Loss (0.368) Accuracy (0.940)\n",
      "44 Epoch 499 iteration - Loss (0.362) Accuracy (0.940)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_22500\n",
      "45 Epoch 499 iteration - Loss (0.364) Accuracy (0.940)\n",
      "46 Epoch 499 iteration - Loss (0.358) Accuracy (0.940)\n",
      "47 Epoch 499 iteration - Loss (0.352) Accuracy (0.940)\n",
      "48 Epoch 499 iteration - Loss (0.351) Accuracy (0.940)\n",
      "49 Epoch 499 iteration - Loss (0.346) Accuracy (0.940)\n",
      "Saved checkpoint cifar10_checkpoints/cifar10_cnn_25000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "with tf.Graph().as_default():\n",
    "    # build input placeholders\n",
    "    images, labels = build_inputs(batch_size, loader.get_image_size())\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = build_model(images, batch_size)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = build_loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op, global_step = build_train_operation(loss, lr_params)\n",
    "    \n",
    "    # Build the accuracy and correct number of examples\n",
    "    # to check model is learned correctly while training\n",
    "    accuracy, correct_num = build_accuracy(logits, labels)\n",
    "\n",
    "    # Create Saver-object to save and reload the model later\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                         log_device_placement=True)) as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Train the model\n",
    "        for ie in range(num_epochs):\n",
    "            for ii in range(iteration_per_epoch):\n",
    "                # Load a batch data\n",
    "                batch = loader.get_batch(batch_size, 'train')\n",
    "\n",
    "                # Run the optimizer\n",
    "                iteration, _ = sess.run([global_step, train_op], \n",
    "                                                     feed_dict={images:batch['images'],\n",
    "                                                                labels:batch['labels']})\n",
    "\n",
    "                # Print the accuracy and loss of current batch data\n",
    "                if iteration % print_frequency == 0:\n",
    "                    batch_loss, batch_acc = sess.run([loss, accuracy], \n",
    "                                                     feed_dict={images:batch['images'],\n",
    "                                                                labels:batch['labels']})\n",
    "                    print('%d Epoch %d iteration - Loss (%.3f) Accuracy (%.3f)' \n",
    "                              %(ie, ii, batch_loss, batch_acc))\n",
    "\n",
    "                # Save checkpoint\n",
    "                if iteration % save_checkpoint_frequency == 0:\n",
    "                    saver.save(sess, save_path=save_path, global_step=global_step)\n",
    "                    print('Saved checkpoint %s_%d' % (save_path, iteration))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last checkpoint path is cifar10_checkpoints/cifar10_cnn-25000\n"
     ]
    }
   ],
   "source": [
    "# Manually set the checkpoint path\n",
    "#checkpoint_path = 'cifar10_checkpoints/cifar10_cnn-5000'\n",
    "# Automatically find the last checkpoint\n",
    "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir='cifar10_checkpoints/')\n",
    "print('Last checkpoint path is %s' % (checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is restored from cifar10_checkpoints/cifar10_cnn-25000\n",
      "Test accuracy: 71.03%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluation model\"\"\"\n",
    "with tf.Graph().as_default():\n",
    "    # build input placeholders\n",
    "    images, labels = build_inputs(batch_size, loader.get_image_size())\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = build_model(images, batch_size)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = build_loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op, global_step = build_train_operation(loss, lr_params)\n",
    "    \n",
    "    # Build the accuracy and correct number of examples\n",
    "    # to check model is learned correctly while training\n",
    "    accuracy, correct_num = build_accuracy(logits, labels)\n",
    "\n",
    "    # Create Saver-object to save and reload the model later\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                         log_device_placement=True)) as sess:\n",
    "        # Load the checkpoint or initialize the variables\n",
    "        if checkpoint_path != '':\n",
    "            saver.restore(sess, save_path=checkpoint_path)\n",
    "            print('Model is restored from %s' % checkpoint_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Evaluate the model\n",
    "        num_correct = 0\n",
    "        num_examples = 0    \n",
    "        while True:\n",
    "            # Load a batch data\n",
    "            batch = loader.get_batch(batch_size, 'test')\n",
    "            if batch['wrapped']: break\n",
    "\n",
    "            # Compute the correct numbers\n",
    "            batch_acc, batch_correct_num = sess.run([accuracy, correct_num], \n",
    "                                                feed_dict={images:batch['images'],\n",
    "                                                           labels:batch['labels']})\n",
    "\n",
    "            num_correct += batch_correct_num\n",
    "            num_examples += batch_size\n",
    "        print('Test accuracy: %.2f%%' % (num_correct / num_examples * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
