{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'utils'))\n",
    "from utils import *\n",
    "from tf_utils import *\n",
    "import cifar10_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TensorFlow tutorial is written based on https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10, which is the tutorial for the image classification task on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic model parameters.\n",
    "batch_size = 128\n",
    "data_dir = 'cifar10_data'\n",
    "data_url = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "IMAGE_SIZE = cifar10_loader.IMAGE_SIZE\n",
    "NUM_CLASSES = cifar10_loader.NUM_CLASSES\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_loader.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_loader.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\n",
    "NUM_EPOCHS_PER_DECAY = 350.0      # Epochs after which learning rate decays.\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n",
    "INITIAL_LEARNING_RATE = 0.1       # Initial learning rate.\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-binary.tar.gz 100.0%('Successfully downloaded', 'cifar-10-binary.tar.gz', 170052171, 'bytes.')\n"
     ]
    }
   ],
   "source": [
    "maybe_download_and_extract(data_url, data_dir, 'cifar-10-batches-bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get variable initialization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initializer = get_initializer('normal', **{'stddev':0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for constructing model (graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_inputs(data_dir, eval_data=False):\n",
    "    \"\"\"Construct input for CIFAR evaluation using the Reader ops.\n",
    "    Args:\n",
    "        data_dir: path to directory including data\n",
    "        eval_data: bool, indicating if one should use the train or eval data set.\n",
    "    Returns:\n",
    "        images: Images. 4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "        labels: Labels. 1D tensor of [batch_size] size.\n",
    "    Raises:\n",
    "        ValueError: If no data_dir\n",
    "    \"\"\"\n",
    "    if not data_dir:\n",
    "        raise ValueError('Please supply a data_dir')\n",
    "    data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
    "    images, labels = cifar10_loader.inputs(eval_data=eval_data,\n",
    "                                        data_dir=data_dir,\n",
    "                                        batch_size=batch_size)\n",
    "    \n",
    "    #images, labels = cifar10_loader.distorted_inputs(data_dir=data_dir,\n",
    "    #                                              batch_size=batch_size)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def build_model(images):\n",
    "    \"\"\"Build the CIFAR-10 model.\n",
    "    Args:\n",
    "        images: Images returned from distorted_inputs() or inputs().\n",
    "    Returns:\n",
    "        Logits.\n",
    "    \"\"\"\n",
    "    # We instantiate all variables using tf.get_variable() instead of\n",
    "    # tf.Variable() in order to share variables across multiple GPU training runs.\n",
    "    # If we only ran this model on a single GPU, we could simplify this function\n",
    "    # by replacing all instances of tf.get_variable() with tf.Variable().\n",
    "    \n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        conv1 = get_conv2D_layer(images, 3, 64, 5, 1, initializer, 0.0, 'relu', scope, True)\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        conv2 = get_conv2D_layer(norm1, 64, 64, 5, 1, initializer, 0.0, 'relu', scope, True)\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "\n",
    "    # fc1\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        fc1 = get_fully_connected_layer(pool2, -1, 384, initializer, 0.004, 'relu', \n",
    "                                        True, batch_size, scope, True)\n",
    "\n",
    "    # fc2\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        fc2 = get_fully_connected_layer(fc1, 384, 192, initializer, 0.004, 'relu', \n",
    "                                        False, -1, scope, True)\n",
    "\n",
    "    # linear layer(WX + b),\n",
    "    # We don't apply softmax here because\n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits accepts the unscaled logits\n",
    "    # and performs the softmax internally for efficiency.\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        softmax_linear = get_fully_connected_layer(fc2, 192, NUM_CLASSES, initializer, \n",
    "                                                   0.0, 'None', False, -1, scope, True)\n",
    "\n",
    "    return softmax_linear\n",
    "\n",
    "\n",
    "def build_loss(logits, labels):\n",
    "    \"\"\"Add L2Loss to all the trainable variables.\n",
    "    Add summary for \"Loss\" and \"Loss/avg\".\n",
    "    Args:\n",
    "        logits: Logits from inference().\n",
    "        labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "            of shape [batch_size]\n",
    "    Returns:\n",
    "        Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    # Calculate the average cross entropy loss across the batch.\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "    # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "    # decay terms (L2 loss).\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "\n",
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "    Generates moving average for all losses and associated summaries for\n",
    "    visualizing the performance of the network.\n",
    "    Args:\n",
    "        total_loss: Total loss from loss().\n",
    "    Returns:\n",
    "        loss_averages_op: op for generating moving averages of losses.\n",
    "    \"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.summary.scalar(l.op.name + ' (raw)', l)\n",
    "        tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op\n",
    "\n",
    "\n",
    "def build_train_operation(total_loss, global_step):\n",
    "    \"\"\"Train CIFAR-10 model.\n",
    "    Create an optimizer and apply to all trainable variables. Add moving\n",
    "    average for all trainable variables.\n",
    "    Args:\n",
    "        total_loss: Total loss from loss().\n",
    "        global_step: Integer Variable counting the number of training steps\n",
    "            processed.\n",
    "    Returns:\n",
    "        train_op: op for training.\n",
    "    \"\"\"\n",
    "    # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = 'cifar10_train'\n",
    "max_steps = 10000\n",
    "log_device_placement = False\n",
    "log_frequency = 10\n",
    "\n",
    "if tf.gfile.Exists(train_dir):\n",
    "    tf.gfile.DeleteRecursively(train_dir)\n",
    "else:\n",
    "    tf.gfile.MakeDirs(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv1/weight_loss (raw) is illegal; using conv1/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv1/weight_loss (raw) is illegal; using conv1/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2/weight_loss (raw) is illegal; using conv2/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv2/weight_loss (raw) is illegal; using conv2/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc1/weight_loss (raw) is illegal; using fc1/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc1/weight_loss (raw) is illegal; using fc1/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc2/weight_loss (raw) is illegal; using fc2/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc2/weight_loss (raw) is illegal; using fc2/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name softmax_linear/weight_loss (raw) is illegal; using softmax_linear/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name softmax_linear/weight_loss (raw) is illegal; using softmax_linear/weight_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into cifar10_train/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1 into cifar10_train/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:04.283500: step 0, loss = 2.45 (3101.3 examples/sec; 0.041 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 22.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 22.3354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:07.876035: step 100, loss = 2.44 (3562.9 examples/sec; 0.036 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 29.5187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.5187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:11.253180: step 200, loss = 2.43 (3790.2 examples/sec; 0.034 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:14.583938: step 300, loss = 2.42 (3843.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:17.913611: step 400, loss = 2.41 (3844.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:21.244854: step 500, loss = 2.40 (3842.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 29.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.9986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:24.578071: step 600, loss = 2.39 (3840.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:27.910379: step 700, loss = 2.39 (3841.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:31.242549: step 800, loss = 2.38 (3841.3 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:34.570937: step 900, loss = 2.38 (3845.7 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:37.887165: step 1000, loss = 2.37 (3859.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:41.208443: step 1100, loss = 2.36 (3854.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 29.9616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.9616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:44.545770: step 1200, loss = 2.36 (3835.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:47.870003: step 1300, loss = 2.36 (3850.5 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:51.182999: step 1400, loss = 2.34 (3863.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:54.496309: step 1500, loss = 2.08 (3863.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:16:57.830224: step 1600, loss = 1.97 (3839.3 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 29.9214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.9214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:01.170653: step 1700, loss = 1.96 (3831.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:04.491874: step 1800, loss = 1.97 (3854.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:07.807444: step 1900, loss = 1.92 (3860.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:11.128983: step 2000, loss = 1.96 (3853.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 29.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.9845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:14.463250: step 2100, loss = 1.71 (3838.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:17.794754: step 2200, loss = 1.64 (3842.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 29.9335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 29.9335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:21.135140: step 2300, loss = 1.54 (3831.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:24.457405: step 2400, loss = 1.52 (3852.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:27.787355: step 2500, loss = 1.34 (3843.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:31.103936: step 2600, loss = 1.34 (3859.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:34.426686: step 2700, loss = 1.54 (3852.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:37.747162: step 2800, loss = 1.37 (3854.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:41.064529: step 2900, loss = 1.24 (3858.5 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:44.369785: step 3000, loss = 1.11 (3872.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:47.663736: step 3100, loss = 1.37 (3885.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:50.976258: step 3200, loss = 1.31 (3864.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:54.284409: step 3300, loss = 1.12 (3869.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:17:57.613291: step 3400, loss = 1.19 (3845.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:00.946104: step 3500, loss = 1.27 (3840.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:04.259411: step 3600, loss = 1.06 (3863.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:07.579475: step 3700, loss = 1.01 (3855.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:10.888304: step 3800, loss = 0.98 (3868.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:14.213827: step 3900, loss = 0.86 (3849.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:17.525346: step 4000, loss = 1.16 (3865.3 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:20.835731: step 4100, loss = 1.01 (3866.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:24.135420: step 4200, loss = 1.02 (3879.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:27.437886: step 4300, loss = 0.82 (3875.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:30.756615: step 4400, loss = 0.95 (3856.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:34.062787: step 4500, loss = 0.82 (3871.5 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:37.375651: step 4600, loss = 0.85 (3863.7 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:40.680451: step 4700, loss = 0.67 (3873.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:43.999941: step 4800, loss = 0.71 (3856.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:47.311649: step 4900, loss = 0.84 (3865.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:50.607525: step 5000, loss = 0.89 (3883.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:53.905142: step 5100, loss = 1.03 (3881.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:18:57.204483: step 5200, loss = 0.89 (3879.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:00.514252: step 5300, loss = 0.62 (3867.3 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:03.826808: step 5400, loss = 0.64 (3864.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:07.125716: step 5500, loss = 0.70 (3880.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.4128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:10.413881: step 5600, loss = 0.65 (3892.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:13.707703: step 5700, loss = 0.84 (3886.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:17.006173: step 5800, loss = 0.62 (3880.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.4263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.4263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:20.291954: step 5900, loss = 0.49 (3895.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.4162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.4162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:23.581072: step 6000, loss = 0.68 (3891.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:26.888901: step 6100, loss = 0.84 (3869.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:30.198169: step 6200, loss = 0.90 (3867.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.4488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.4488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:33.482299: step 6300, loss = 0.61 (3897.5 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:36.778589: step 6400, loss = 0.62 (3883.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:40.075861: step 6500, loss = 0.77 (3882.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:43.387980: step 6600, loss = 0.92 (3864.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:46.704839: step 6700, loss = 0.62 (3859.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:50.027795: step 6800, loss = 0.68 (3852.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:53.334093: step 6900, loss = 0.75 (3871.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:56.644067: step 7000, loss = 0.64 (3867.1 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:19:59.956717: step 7100, loss = 0.44 (3864.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:03.268152: step 7200, loss = 0.53 (3865.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:06.585252: step 7300, loss = 0.62 (3858.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:09.896367: step 7400, loss = 0.67 (3865.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:13.205332: step 7500, loss = 0.59 (3868.3 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:16.519963: step 7600, loss = 0.58 (3861.7 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:19.834289: step 7700, loss = 0.55 (3862.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:23.139527: step 7800, loss = 0.62 (3872.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:26.432179: step 7900, loss = 0.55 (3887.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:29.741506: step 8000, loss = 0.57 (3867.9 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:33.060503: step 8100, loss = 0.57 (3856.6 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:36.382116: step 8200, loss = 0.56 (3853.5 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:39.699013: step 8300, loss = 0.62 (3859.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:43.004599: step 8400, loss = 0.64 (3872.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:46.312753: step 8500, loss = 0.59 (3869.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:49.617669: step 8600, loss = 0.52 (3873.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:52.926157: step 8700, loss = 0.48 (3868.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:56.227848: step 8800, loss = 0.50 (3876.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:20:59.527255: step 8900, loss = 0.52 (3879.5 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:02.832865: step 9000, loss = 0.56 (3872.2 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:06.138281: step 9100, loss = 0.84 (3872.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:09.430602: step 9200, loss = 0.51 (3887.8 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:12.723062: step 9300, loss = 0.55 (3887.7 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:16.036177: step 9400, loss = 0.56 (3863.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.1043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:19.357093: step 9500, loss = 0.50 (3854.4 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:22.656933: step 9600, loss = 0.55 (3879.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:25.947381: step 9700, loss = 0.76 (3890.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.3464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.3464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:29.242728: step 9800, loss = 0.44 (3884.3 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 30.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 30.2834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-06-16 02:21:32.545947: step 9900, loss = 0.48 (3875.0 examples/sec; 0.033 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into cifar10_train/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 10000 into cifar10_train/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
    "    # GPU and resulting in a slow down.\n",
    "    with tf.device('/cpu:0'):\n",
    "        images, labels = build_inputs(data_dir, False)\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = build_model(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = build_loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = build_train_operation(loss, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "        \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "        def begin(self):\n",
    "            self._step = -1\n",
    "            self._start_time = time.time()\n",
    "\n",
    "        def before_run(self, run_context):\n",
    "            self._step += 1\n",
    "            return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "        def after_run(self, run_context, run_values):\n",
    "            if self._step % log_frequency == 0:\n",
    "                current_time = time.time()\n",
    "                duration = current_time - self._start_time\n",
    "                self._start_time = current_time\n",
    "\n",
    "                loss_value = run_values.results\n",
    "                examples_per_sec = log_frequency * batch_size / duration\n",
    "                sec_per_batch = float(duration / log_frequency)\n",
    "\n",
    "                format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                    'sec/batch)')\n",
    "                print (format_str % (datetime.now(), self._step, loss_value,\n",
    "                    examples_per_sec, sec_per_batch))\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession(checkpoint_dir=train_dir,\n",
    "            hooks=[tf.train.StopAtStepHook(last_step=max_steps),\n",
    "            tf.train.NanTensorHook(loss), _LoggerHook()],\n",
    "            config=tf.ConfigProto(log_device_placement=log_device_placement)) as mon_sess:\n",
    "        while not mon_sess.should_stop():\n",
    "            mon_sess.run(train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_dir = 'cifar10_eval'\n",
    "eval_data = 'test'\n",
    "checkpoint_dir = 'cifar10_train'\n",
    "eval_interval_secs = 1\n",
    "num_examples = 10000\n",
    "run_once = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_once(saver, summary_writer, top_k_op, summary_op):\n",
    "    \"\"\"Run Eval once.\n",
    "    Args:\n",
    "    saver: Saver.\n",
    "    summary_writer: Summary writer.\n",
    "    top_k_op: Top K op.\n",
    "    summary_op: Summary op.\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        print(ckpt)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            # Restores from checkpoint\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            # Assuming model_checkpoint_path looks something like:\n",
    "            #   /my-favorite-path/cifar10_train/model.ckpt-0,\n",
    "            # extract global_step from it.\n",
    "            global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "        else:\n",
    "            print('No checkpoint file found')\n",
    "            return\n",
    "\n",
    "        # Start the queue runners.\n",
    "        coord = tf.train.Coordinator()\n",
    "        try:\n",
    "            threads = []\n",
    "            for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "                threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "                                                 start=True))\n",
    "\n",
    "            num_iter = int(math.ceil(num_examples / batch_size))\n",
    "            true_count = 0  # Counts the number of correct predictions.\n",
    "            total_sample_count = num_iter * batch_size\n",
    "            step = 0\n",
    "            while step < num_iter and not coord.should_stop():\n",
    "                predictions = sess.run([top_k_op])\n",
    "                true_count += np.sum(predictions)\n",
    "                step += 1\n",
    "\n",
    "            # Compute precision @ 1.\n",
    "            precision = true_count / total_sample_count\n",
    "            print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
    "\n",
    "            summary = tf.Summary()\n",
    "            summary.ParseFromString(sess.run(summary_op))\n",
    "            summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
    "            summary_writer.add_summary(summary, global_step)\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            coord.request_stop(e)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads, stop_grace_period_secs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:18.671515: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:24.496865: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:30.323077: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:36.167443: precision @ 1 = 0.995\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:41.996135: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:47.834016: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:53.677403: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:01:59.479715: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:05.345610: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:11.370059: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:17.228119: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:23.061125: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:28.912307: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:34.772374: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:40.653079: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:46.501053: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:52.350772: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:02:58.163311: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:03:03.967734: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:03:09.804742: precision @ 1 = 0.997\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "2017-06-16 03:03:15.791950: precision @ 1 = 0.996\n",
      "model_checkpoint_path: \"cifar10_train/model.ckpt-10000\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-1\"\n",
      "all_model_checkpoint_paths: \"cifar10_train/model.ckpt-10000\"\n",
      "\n",
      "INFO:tensorflow:Error reported to Coordinator: <type 'exceptions.RuntimeError'>, Attempted to use a closed Session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <type 'exceptions.RuntimeError'>, Attempted to use a closed Session.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-7813319c709b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0meval_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_k_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_once\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-3538ad5aebe9>\u001b[0m in \u001b[0;36meval_once\u001b[1;34m(saver, summary_writer, top_k_op, summary_op)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum_iter\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcoord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtop_k_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[0mtrue_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    eval_data = eval_data == 'test'\n",
    "    images, labels = build_inputs(data_dir, eval_data)\n",
    "    \n",
    "    logits = build_model(images)\n",
    "    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "    \n",
    "    # Restore the moving average version of the learned variables for eval.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "    variables_to_restore = variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(eval_dir, g)\n",
    "\n",
    "    while True:\n",
    "        eval_once(saver, summary_writer, top_k_op, summary_op)\n",
    "        if run_once:\n",
    "            break\n",
    "        time.sleep(eval_interval_secs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
