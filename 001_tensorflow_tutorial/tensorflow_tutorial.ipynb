{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tensorflow Tutorial\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph Construction and Session\n",
    "- Tensorflow는 기본적으로 값과 연산을 가지는 node들로 구성된 Directed Acyclic Graph를 형성하여 구현되는 구조를 가진다. \n",
    "\n",
    "### 1.1. tf.constant() & tf.Session()\n",
    "- tf.constant()는 변하지 않은 값을 가지는 node를 생성한다.\n",
    "- tf.constant(value, dtype=None, shape=None, name=\"Const', verify_shape=False)\n",
    "- tf.sesstion()은 형성된 graph를 GPU/CPU에 올려 실행(계산)하는 역할을 한다.\n",
    "\n",
    "#### 1.1.1. compute '3+4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3과 4를 담는 node를 각각 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.Session 을 통해서 graph의 node를 run하면 node 값을 계산하고 결과를 return된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print \"node3: \",node3\n",
    "print \"sess.run(node3): \",sess.run(node3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1.1.2. Compute ‘((3+4)^2)*5’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0, tf.float32)\n",
    "node3 = tf.add(node1, node2)\n",
    "node4 = tf.constant(2.0, tf.float32)\n",
    "node5 = tf.pow(node3, node4)\n",
    "node6 = tf.constant(5.0, tf.float32)\n",
    "node7 = tf.multiply(node5, node6)\n",
    "\n",
    "print \"3 + 4 = \",sess.run(node3)\n",
    "print \"(3 + 4)^2 = \",sess.run(node5)\n",
    "print \"((3 + 4)^2) * 5 = \",sess.run(node7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.2. tf.placeholder()\n",
    "- tf.placeholder는 graph를 run할 때 값을 입력해주어야 하는 node이다.\n",
    "- 함수의 input 즉 f(x)에서 x의 역할과도 같다.\n",
    "\n",
    "#### 1.2.1. Compute 'x+y'\n",
    "- tf.placeholder를 이용하여 sess.run할때마다 x,y에 input값을 주어 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "adder = x + y\n",
    "\n",
    "z1 = sess.run(adder, {x: 3, y: 4.5})\n",
    "print(z1)\n",
    "\n",
    "z2 = sess.run(adder, {x: [1,3], y: [2, 4]})\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2.2. Compute '||Ax-y||'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "A = tf.constant([[1., 2.], [3., 4.]])\n",
    "y_ = tf.matmul(A,x)\n",
    "diff = y - y_\n",
    "err = tf.norm(diff)\n",
    "\n",
    "# should pass x to 2D matrix shape\n",
    "print \"A*x = \\n\",sess.run(y_, {x: [[1.], [2.]]})\n",
    "print \"|Ax-y| = \\n\",sess.run(err, {x: [[1.], [2.]], y:[[8.], [7.]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Variables\n",
    "- tf.Variable()는 tf.constant()와 같이 값이 고정된 것이 아니고 값을 변경할 수 있는 node이다.\n",
    "- 예를들어, f(x) = 3*x^2 + 2*x + 1 라 했을 때 x는 placeholder이고 3,2,1은 값을 변경할 상수라 하면 tf.constant이다.\n",
    "- 그런데, f(x) = a*x^2 + b*x + c 에서 필요에 따라 a,b,c 값을 변경해줄 수 있다면, 이들은 tf.Variable로 정의한다.\n",
    "- tf.Variable은 결국 function(graph)의 parameter이고, learning이 가능하다.\n",
    "- tf.Variable(initial_value, name=optional_name)\n",
    "\n",
    "### 2.1. Learning a Linear Model\n",
    "- 이 chapter에서는 주어진 데이터로부터 함수 f(x)를 선형회귀하는 모델 Wx+b 를 learning한다. \n",
    "- Initializer는 텐서플로우 variable들의 정의된 초기값을 부여해준다. \n",
    "- tf.global_initializer는 graph 내의 정의된 모든 variable을 초기화시키는 연산자.\n",
    "- 초기화 연산자를 sess.run()으로 실행시켜 초기화해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], tf.float32)  # Variable adds learnable parameters in graph\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sess.run에 계산할 node 이름과 거기에 필요한 placeholder 값을 주어 결과를 뱉는다.\n",
    "- 하지만, variable들을 initialize하지 않고 graph를 run하면 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# without initializing variable, running graph causes error\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- global_variables_initializer로 graph에 정의된 모든 variable들을 초기화하고 다시 시도하면 성공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run variable initializer op, before running the graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Then, it works\n",
    "print(sess.run(linear_model, {x:[1,2,3,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss 함수는 squared error function 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss\n",
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "\n",
    "print \"loss : \",sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model을 training하기 전에, variable의 값 변경하는 법을 소개..\n",
    "- tf.assign(variable, modified_value) 하여 variable의 값을 modified_value로 바꾸는 연산자 생성하고\n",
    "- sess.run()에 넘겨 값을 바꾼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assign different value to the variables\n",
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])   # These are optimal parameters\n",
    "\n",
    "print \"Before assigning, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]\n",
    "sess.run([fixW, fixb])\n",
    "print \"After assigning, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]\n",
    "\n",
    "print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializer을 다시 run하면 처음에 정의된 값으로 초기화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Go back to the initial value\n",
    "sess.run(init)\n",
    "print \"Ater initializing again, W : \",sess.run(W)[0], \"b : \",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.train에 정의된 optimizer중 하나를 이용하여 variable들을 learning한다.\n",
    "- 예시에서는 일반적인 Stochastic Gradient Descent (SGD) optimizer 이용.\n",
    "- Optimizer는 loss를 minimizing하는 방향으로 배우도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learn the model using optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "for i in range(1000):\n",
    "    _, l = sess.run([train, loss], {x:[1,2,3,4], y:[0,-1,-2,-3]})\n",
    "    if (i+1) % 100 == 0:\n",
    "        print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    \n",
    "print \"Learned model parameters, \\t W : \",sess.run(W)[0],\", b : \",sess.run(b)[0]\n",
    "print \"Optimal parameters are,   \\t W :  -1 \\t, b :  1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2. Initializing Variables\n",
    "- 여러가지 initialize 방법들 소개\n",
    "\n",
    "#### 2.2.1. tf.global_variable_initializer()\n",
    "- model의 모든 variable 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable([.2], tf.float32)\n",
    "W2 = tf.Variable([.4], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 tf.variables_initialize()\n",
    "- tf.variables_initialize(var_list)는 var_list (variables들의 list)에 포함된 variables들만 initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 위에 정의된 variable들의 값을 바꾼다.\n",
    "assn_W1 = W1.assign([.1])\n",
    "assn_W2 = W2.assign([.1])\n",
    "assn_b = b.assign([-.1])\n",
    "\n",
    "sess.run([assn_W1, assn_W2, assn_b])\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# W1, b만 초기값으로 initialize한다.\n",
    "init_W = tf.variables_initializer([W1, b])\n",
    "# init_W = tf.variables_initializer(set(tf.global_variables())-W2)\n",
    "sess.run(init_W)\n",
    "print \"W1 :\",sess.run(W1)[0],\",\\t W2 :\",sess.run(W2)[0],\",\\t b :\",sess.run(b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Variable.initialized_value()\n",
    "- 두 변수 A, B를 동일한 값으로 초기화시킬 때, Variable의 initialized_value()를 이용한다.\n",
    "- initialized_value는 그 변수의 초기화 값을 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = tf.Variable(tf.random_normal([2,3], stddev=0.35), name=\"A\")\n",
    "# A와 같은 초기 값을 가지는 변수 B를 만든다.\n",
    "B = tf.Variable(A.initialized_value(), name=\"B\")\n",
    "# A의 초기값의 정확히 2배의 초기값을 가지는 변수 C를 만든다.\n",
    "C = tf.Variable(A.initialized_value() * 2.0, name=\"C\")\n",
    "\n",
    "init_ABC = tf.variables_initializer(set([A,B,C]))\n",
    "sess.run(init_ABC)\n",
    "print \"A :\\n\",sess.run(A),\"\\nB :\\n\",sess.run(B),\"\\nC :\\n\",sess.run(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Saving and Loading Variables\n",
    "- 배운 모델을 저장하거나, 배워진 모델을 읽어들일 때 필요한 과정.\n",
    "- tf.train.Saver()을 사용한다.\n",
    "- 모든 변수가 아니라, 지정한 몇 개의 변수만 저장하고 불러올 수도 있다.\n",
    "- key : 저장/불러올 변수 이름의 이름, value : 저장/불러올 변수 를 가지는 python dictionary를 만들어서 train.Saver의 input으로 넘긴다.\n",
    "- train.Saver의 input이 없을 때는 default로 graph 내의 모든 변수들을 save/restore한다. \n",
    "- graph 내의 모든 변수들을 보려면 global_variables() 함수를 사용하여, print(global_bvariables()) 등을 이용.\n",
    "\n",
    "#### 2.4.1 Save model : all variables\n",
    "- tf.train.Saver.save()를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable 생성\n",
    "v1 = tf.Variable(tf.random_normal([2,2]), name=\"v1\")\n",
    "v2 = tf.Variable(tf.random_normal([2,1]), name=\"v2\")\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# tf.train.Saver() 을 먼저 생성. 저장할 변수들 pass \n",
    "saver = tf.train.Saver({\"w1\": v1, \"w2\": v2})\n",
    "# saver = tf.train.Saver([v1, v2])   # name은 default(v.op.name)으로 들어감\n",
    "# saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
    "\n",
    "# 모델 생성, 변수 초기화, training, 모델 저장.\n",
    "with tf.Session() as sess:\n",
    "    # model 생성\n",
    "    loss = tf.norm(tf.matmul(v1,x)+v2-y)\n",
    "    \n",
    "    # 변수 초기화\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # training\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    for i in range(200):\n",
    "        _, l = sess.run([train, loss], {x:[[1.,2.,3.,4.],[0.,1.,2.,3.]], y:[[0.,-1.,-2.,-3.],[-1.,-2.,-3.,-4.]]})\n",
    "        if (i+1) % 20 == 0:\n",
    "            print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    \n",
    "    # 디스크에 변수 저장\n",
    "    save_path = saver.save(sess, \"model.ckpt\")  # 세션과 저장 파일 이름 지정\n",
    "    print \"v1 :\\n\",sess.run(v1), \"\\nv2 :\\n\", sess.run(v2)\n",
    "    print(\"Model (variables v1, v2) saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Load model : all variables\n",
    "- 변수들을 저장된 값을 불러 초기화한다. \n",
    "- 이 경우 restore된 변수들은 따로 initializer로 초기화할 필요 없다.\n",
    "- tf.train.Saver.restore()를 사용\n",
    "- 밑의 예시는 위 예시에서 저장된 checkpoint부터 시작하여 training을 이어나가는 과정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable 생성\n",
    "w1 = tf.Variable(tf.random_normal([2,2]), name=\"w1\")\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name=\"w2\")\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# restorer를 정의하여 restore할 \n",
    "restorer = tf.train.Saver({\"w1\": w1, \"w2\": w2})\n",
    "with tf.Session() as sess:\n",
    "    # model 생성\n",
    "    loss = tf.norm(tf.matmul(w1,x)+w2-y)\n",
    "    \n",
    "    # checkpoint 파일로부터 변수값 읽어와서 변수 초기화\n",
    "    restorer.restore(sess, \"model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    print \"w1 :\\n\",sess.run(w1), \"\\nw2 :\\n\", sess.run(w2)\n",
    "    \n",
    "    # training\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "    for i in range(200):\n",
    "        _, l = sess.run([train, loss], {x:[[1.,2.,3.,4.],[0.,1.,2.,3.]], y:[[0.,-1.,-2.,-3.],[-1.,-2.,-3.,-4.]]})\n",
    "        if (i+1) % 20 == 0:\n",
    "            print \"iteration : \", i+1 ,\" loss : \", l\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- restore되지 않은 변수는 꼭 초기화를 해준다.화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w3 = tf.Variable(tf.random_normal([2,1]), name=\"w3\")\n",
    "w4 = tf.Variable(tf.random_normal([2,1]), name=\"w4\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    restorer.restore(sess, \"model.ckpt\")\n",
    "    print(\"Initialize w1, w2 by restoring saved values\")\n",
    "\n",
    "    # restore 되지 않은 w3는 초기화\n",
    "    init_34 = tf.variables_initializer([w3, w4])\n",
    "    sess.run(init_34)\n",
    "    \n",
    "    print \"w1 : \",sess.run(w1), \"\\nw2 : \", sess.run(w2)\n",
    "    print \"\\nw3 : \",sess.run(w3), \"\\nw4 : \", sess.run(w4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sharing Variables\n",
    "- Variable은 선언할 때 마다 새로 생성되어 덮어씌어진다는 특징이 있는데, 이미 만든 variable은 새로 만들지 않고 기존의 것을 재사용하게 하도록 하는 방법이 필요할 때가 있다.\n",
    "- 밑의 예시는 convolution layer를 포함한 model인데, 두가지의 문제점이 있다.\n",
    "    - 첫째로, 함수 내의 모델이 고정되어있어 모델의 layer를 추가하려면 새로운함수를 만들거나, main함수에 직접 넣어주어야 하는 비효율이 발생한다.\n",
    "    - 둘째로, 모델에 input을 넣어줄 때마다 새로운 variable이 생성되어, input에 따라 다른 filter를 통과하게."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_image_filter(input_images):\n",
    "    conv1_weights = tf.Variable(tf.random_normal([5, 5, 3, 32]), name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    conv1 = tf.nn.conv2d(input_images, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1 + conv1_biases)\n",
    "\n",
    "    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]), name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")\n",
    "    conv2 = tf.nn.conv2d(relu1, conv2_weights,  strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2 + conv2_biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "\n",
    "image1 = tf.expand_dims(tf.constant(misc.imread('acoustic-guitar-player.jpg'), tf.float32), 0)\n",
    "image2 = tf.expand_dims(tf.constant(misc.imread('iris.jpg'), tf.float32), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 언급한 두 번째 문제 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = tf.global_variables()\n",
    "# First call creates one set of 4 variables.\n",
    "result1 = my_image_filter(image1)\n",
    "print \"Number of created variables : \\n\",len(set(tf.global_variables())-set(temp))\n",
    "\n",
    "# Another set of 4 variables is created in the second call.\n",
    "result2 = my_image_filter(image2)\n",
    "print \"Number of created variables : \\n\",len(set(tf.global_variables())-set(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.placeholder를 이용하여 sess.run 할때마다 input을 넘겨주거나, variable들을 dictionary로 따로 함수 밖에 만들어놓은 뒤 함수에서 refer하면 두 번째 문제는 해결되지만, 첫번째 문제는 여전히 해결하지 못한다.\n",
    "- tf.variable_scope 와 tf.get_variable을 이용하여 두 문제를 효율적으로 해결해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. tf.get_variable() & tf.variable_scope()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = tf.get_variable(\"v\", [1])\n",
    "v1 = tf.get_variable(\"v\", [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"foo\"):                  # tf.get_variable_scope().reuse == False\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "    tf.get_variable_scope().reuse_variables()   # tf.get_variable_scope().reuse == True\t\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 is v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fooo/v:0\n",
      "fooo/bar/v:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"fooo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "print(v.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"fooo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "print(v.name)\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"gooo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(\"gooo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 is v     # v1 = v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuse root ?    False\n",
      "reuse root/foo ?    False\n",
      "reuse root/foo ?    True\n",
      "reuse root/foo/bar ?    True\n",
      "reuse root ?    False\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"root\"):\n",
    "    # At start, the scope is not reusing.\n",
    "    print \"reuse root ?   \", tf.get_variable_scope().reuse\n",
    "    with tf.variable_scope(\"foo\"):\n",
    "        # Opened a sub-scope, still not reusing.\n",
    "        print \"reuse root/foo ?   \", tf.get_variable_scope().reuse\n",
    "    with tf.variable_scope(\"foo\", reuse=True):\n",
    "        # Explicitly opened a reusing scope.\n",
    "        print \"reuse root/foo ?   \", tf.get_variable_scope().reuse\n",
    "        with tf.variable_scope(\"bar\"):\n",
    "            # Now sub-scope inherits the reuse flag.\n",
    "            print \"reuse root/foo/bar ?   \", tf.get_variable_scope().reuse\n",
    "    # Exited the reusing scope, back to a non-reusing one.\n",
    "    print \"reuse root ?   \", tf.get_variable_scope().reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Foo\") as foo_scope:\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(foo_scope):\n",
    "    w = tf.get_variable(\"w\", [1])\n",
    "with tf.variable_scope(foo_scope, reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "    w1 = tf.get_variable(\"w\", [1])\n",
    "assert v1 is v\n",
    "assert w1 is w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Goo\") as foo_scope:\n",
    "    assert foo_scope.name == \"Goo\"\n",
    "with tf.variable_scope(\"bar\"):\n",
    "    with tf.variable_scope(\"baz\") as other_scope:\n",
    "        assert other_scope.name == \"bar/baz\"\n",
    "        with tf.variable_scope(foo_scope) as foo_scope2:\n",
    "            assert foo_scope2.name == \"Goo\"  # Not changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ff1ea447fa07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hoo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hoo/v:0 : %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Default initializer as set above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hoo/w:0 : %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Specific initializer overrides the default.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m    457\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \"\"\"\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minitialized_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \"\"\"\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3725\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   3728\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"hoo\", initializer=tf.constant_initializer(0.4)):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "    print(\"hoo/v:0 : %f\" % v.eval())  # Default initializer as set above.\n",
    "    w = tf.get_variable(\"w\", [1], initializer=tf.constant_initializer(0.3))\n",
    "    print(\"hoo/w:0 : %f\" % w.eval())  # Specific initializer overrides the default.\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        print(\"hoo/bar/v:0 : %f\" % v.eval())  # Inherited default initializer.\n",
    "    with tf.variable_scope(\"baz\", initializer=tf.random_normal_initializer()):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        print(\"hoo/baz/v:0 : %f\" % v.eval())  # Changed default initializer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Solving the previous problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\", “biases”.\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, \tinitializer=tf.random_normal_initializer())\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 3, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable conv1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-14037045eee3>\", line 3, in conv_relu\n    weights = tf.get_variable(\"weights\", kernel_shape, \tinitializer=tf.random_normal_initializer())\n  File \"<ipython-input-3-14037045eee3>\", line 11, in my_image_filter\n    relu1 = conv_relu(input_images, [5, 5, 3, 32], [32])\n  File \"<ipython-input-4-1fb90c781401>\", line 1, in <module>\n    result1 = my_image_filter(image1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1fb90c781401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_image_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_image_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-14037045eee3>\u001b[0m in \u001b[0;36mmy_image_filter\u001b[0;34m(input_images)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Variables created here will be named \"conv1/weights\", \"conv1/biases\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mrelu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Variables created here will be named \"conv2/weights\", \"conv2/biases\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-14037045eee3>\u001b[0m in \u001b[0;36mconv_relu\u001b[0;34m(input, kernel_shape, bias_shape)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconv_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Create variable named \"weights\", “biases”.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"biases\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/daniel/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable conv1/weights already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-14037045eee3>\", line 3, in conv_relu\n    weights = tf.get_variable(\"weights\", kernel_shape, \tinitializer=tf.random_normal_initializer())\n  File \"<ipython-input-3-14037045eee3>\", line 11, in my_image_filter\n    relu1 = conv_relu(input_images, [5, 5, 3, 32], [32])\n  File \"<ipython-input-4-1fb90c781401>\", line 1, in <module>\n    result1 = my_image_filter(image1)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"image_filters\") as scope:\n",
    "    result1 = my_image_filter(image1)\n",
    "    scope.reuse_variables()\n",
    "    result2 = my_image_filter(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Name Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noo/add\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"noo\"):\n",
    "    x = 1.0 + tf.get_variable(\"v\", [1])\n",
    "print(x.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v name : moo/v:0\n",
      "x operation name : moo/bar/add\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"moo\"):\n",
    "    \twith tf.name_scope(\"bar\"):\n",
    "        \t\tv = tf.get_variable(\"v\", [1])\n",
    "        \t\tx = 1.0 + v\n",
    "print(\"v name : %s\" % v.name)\n",
    "print(\"x operation name : %s\" % x.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
